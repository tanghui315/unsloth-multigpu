"""
DDP Manager - PyTorchåŸç”Ÿåˆ†å¸ƒå¼è®­ç»ƒç®¡ç†å™¨
åŸºäºPyTorch DistributedDataParallelå®ç°çœŸæ­£çš„å¹¶è¡Œè®­ç»ƒ
"""

import logging
import os
import socket
from contextlib import contextmanager
from typing import Any, Dict, Optional

import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP

logger = logging.getLogger(__name__)


class DDPManager:
    """
    DDP Manager - åŸºäºPyTorchåŸç”ŸDDPçš„åˆ†å¸ƒå¼è®­ç»ƒç®¡ç†å™¨
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. åˆ†å¸ƒå¼è¿›ç¨‹ç»„åˆå§‹åŒ–
    2. DDPæ¨¡å‹åŒ…è£…å’Œç®¡ç†
    3. è¿›ç¨‹é—´é€šä¿¡åè°ƒ
    4. é”™è¯¯å¤„ç†å’Œèµ„æºæ¸…ç†
    """
    
    def __init__(self, backend: str = "nccl", timeout_minutes: int = 30):
        """
        åˆå§‹åŒ–DDPç®¡ç†å™¨
        
        Args:
            backend: åˆ†å¸ƒå¼åç«¯ï¼ŒGPUæ¨èncclï¼ŒCPUæ¨ègloo
            timeout_minutes: åˆ†å¸ƒå¼æ“ä½œè¶…æ—¶æ—¶é—´
        """
        self.backend = backend
        self.timeout = timeout_minutes * 60  # è½¬æ¢ä¸ºç§’
        
        # åˆ†å¸ƒå¼çŠ¶æ€
        self.is_initialized = False
        self.rank = None
        self.local_rank = None
        self.world_size = None
        self.is_master = False
        
        # DDPæ¨¡å‹å®ä¾‹
        self.ddp_model = None
        self.original_model = None
        
        logger.info(f"ğŸ”§ åˆå§‹åŒ–DDPManager: backend={backend}, timeout={timeout_minutes}min")
    
    def init_process_group(self, rank: int, world_size: int, 
                          master_addr: str = "localhost", 
                          master_port: str = "12355") -> bool:
        """
        åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
        
        Args:
            rank: å½“å‰è¿›ç¨‹åœ¨å…¨å±€çš„æ’å
            world_size: æ€»è¿›ç¨‹æ•°
            master_addr: ä¸»èŠ‚ç‚¹åœ°å€
            master_port: ä¸»èŠ‚ç‚¹ç«¯å£
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆå§‹åŒ–
        """
        try:
            # è®¾ç½®ç¯å¢ƒå˜é‡
            os.environ['MASTER_ADDR'] = master_addr
            os.environ['MASTER_PORT'] = master_port
            os.environ['WORLD_SIZE'] = str(world_size)
            os.environ['RANK'] = str(rank)
            
            # è®¡ç®—local_rankï¼ˆå•æœºå¤šGPUåœºæ™¯ï¼‰
            local_rank = rank % torch.cuda.device_count()
            os.environ['LOCAL_RANK'] = str(local_rank)
            
            # åˆå§‹åŒ–è¿›ç¨‹ç»„
            dist.init_process_group(
                backend=self.backend,
                rank=rank,
                world_size=world_size,
                timeout=torch.distributed.default_pg_timeout if self.timeout == 1800 else 
                       torch.distributed.init_process_group.__defaults__[3]
            )
            
            # è®¾ç½®CUDAè®¾å¤‡
            if torch.cuda.is_available():
                torch.cuda.set_device(local_rank)
            
            # æ›´æ–°çŠ¶æ€
            self.rank = rank
            self.local_rank = local_rank
            self.world_size = world_size
            self.is_master = (rank == 0)
            self.is_initialized = True
            
            if self.is_master:
                logger.info(f"âœ… DDPè¿›ç¨‹ç»„åˆå§‹åŒ–æˆåŠŸ: rank={rank}, world_size={world_size}, backend={self.backend}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ DDPè¿›ç¨‹ç»„åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def wrap_model(self, model: torch.nn.Module, 
                   device_ids: Optional[list] = None,
                   find_unused_parameters: bool = False,
                   broadcast_buffers: bool = True) -> torch.nn.Module:
        """
        ç”¨DDPåŒ…è£…æ¨¡å‹
        
        Args:
            model: è¦åŒ…è£…çš„æ¨¡å‹
            device_ids: è®¾å¤‡IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ¨æ–­
            find_unused_parameters: æ˜¯å¦æŸ¥æ‰¾æœªä½¿ç”¨çš„å‚æ•°
            broadcast_buffers: æ˜¯å¦å¹¿æ’­buffer
            
        Returns:
            torch.nn.Module: DDPåŒ…è£…åçš„æ¨¡å‹
        """
        if not self.is_initialized:
            raise RuntimeError("DDPè¿›ç¨‹ç»„æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨init_process_group()")
        
        # ç§»åŠ¨æ¨¡å‹åˆ°å½“å‰GPU
        if torch.cuda.is_available():
            device = f"cuda:{self.local_rank}"
            model = model.to(device)
        
        # è‡ªåŠ¨æ¨æ–­device_ids
        if device_ids is None and torch.cuda.is_available():
            device_ids = [self.local_rank]
        
        # åŒ…è£…æ¨¡å‹
        try:
            self.original_model = model
            self.ddp_model = DDP(
                model,
                device_ids=device_ids,
                find_unused_parameters=find_unused_parameters,
                broadcast_buffers=broadcast_buffers
            )
            
            if self.is_master:
                param_count = sum(p.numel() for p in model.parameters())
                logger.info(f"âœ… æ¨¡å‹DDPåŒ…è£…å®Œæˆ: {param_count:,} å‚æ•°")
            
            return self.ddp_model
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹DDPåŒ…è£…å¤±è´¥: {e}")
            raise
    
    def create_data_sampler(self, dataset, shuffle: bool = True):
        """
        åˆ›å»ºåˆ†å¸ƒå¼æ•°æ®é‡‡æ ·å™¨
        
        Args:
            dataset: æ•°æ®é›†
            shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
            
        Returns:
            DistributedSampler: åˆ†å¸ƒå¼é‡‡æ ·å™¨
        """
        from torch.utils.data.distributed import DistributedSampler
        
        return DistributedSampler(
            dataset,
            num_replicas=self.world_size,
            rank=self.rank,
            shuffle=shuffle
        )
    
    @contextmanager
    def no_sync(self):
        """
        æš‚åœæ¢¯åº¦åŒæ­¥çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        ç”¨äºæ¢¯åº¦ç´¯ç§¯åœºæ™¯
        """
        if self.ddp_model is not None:
            with self.ddp_model.no_sync():
                yield
        else:
            yield
    
    def barrier(self):
        """è¿›ç¨‹åŒæ­¥å±éšœ"""
        if self.is_initialized:
            dist.barrier()
    
    def all_reduce(self, tensor: torch.Tensor, op=dist.ReduceOp.SUM):
        """
        è·¨è¿›ç¨‹tensorå½’çº¦æ“ä½œ
        
        Args:
            tensor: è¦å½’çº¦çš„tensor
            op: å½’çº¦æ“ä½œç±»å‹
        """
        if self.is_initialized:
            dist.all_reduce(tensor, op=op)
        return tensor
    
    def broadcast(self, tensor: torch.Tensor, src: int = 0):
        """
        ä»æºè¿›ç¨‹å¹¿æ’­tensoråˆ°æ‰€æœ‰è¿›ç¨‹
        
        Args:
            tensor: è¦å¹¿æ’­çš„tensor
            src: æºè¿›ç¨‹rank
        """
        if self.is_initialized:
            dist.broadcast(tensor, src=src)
        return tensor
    
    def cleanup(self):
        """æ¸…ç†åˆ†å¸ƒå¼èµ„æº"""
        try:
            if self.is_initialized:
                dist.destroy_process_group()
                self.is_initialized = False
                
                if self.is_master:
                    logger.info("âœ… DDPè¿›ç¨‹ç»„å·²æ¸…ç†")
                    
        except Exception as e:
            logger.warning(f"âš ï¸ DDPæ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°è­¦å‘Š: {e}")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–DDPçŠ¶æ€ä¿¡æ¯"""
        return {
            'initialized': self.is_initialized,
            'backend': self.backend,
            'rank': self.rank,
            'local_rank': self.local_rank,
            'world_size': self.world_size,
            'is_master': self.is_master,
            'has_ddp_model': self.ddp_model is not None,
            'cuda_available': torch.cuda.is_available(),
            'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºè¢«æ¸…ç†"""
        if self.is_initialized:
            self.cleanup()


def find_free_port() -> str:
    """å¯»æ‰¾ç©ºé—²ç«¯å£"""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('', 0))
        return str(s.getsockname()[1])


def setup_ddp_environment(num_gpus: int, 
                         master_addr: str = "localhost",
                         master_port: Optional[str] = None) -> Dict[str, str]:
    """
    è®¾ç½®DDPç¯å¢ƒå˜é‡
    
    Args:
        num_gpus: GPUæ•°é‡
        master_addr: ä¸»èŠ‚ç‚¹åœ°å€
        master_port: ä¸»èŠ‚ç‚¹ç«¯å£ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨å¯»æ‰¾
        
    Returns:
        Dict: ç¯å¢ƒå˜é‡å­—å…¸
    """
    if master_port is None:
        master_port = find_free_port()
    
    env_vars = {
        'MASTER_ADDR': master_addr,
        'MASTER_PORT': master_port,
        'WORLD_SIZE': str(num_gpus),
        'NCCL_DEBUG': 'INFO',  # è°ƒè¯•ä¿¡æ¯
        'CUDA_VISIBLE_DEVICES': ','.join(str(i) for i in range(num_gpus))
    }
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    for key, value in env_vars.items():
        os.environ[key] = value
    
    logger.info(f"ğŸ”§ DDPç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ: {env_vars}")
    return env_vars