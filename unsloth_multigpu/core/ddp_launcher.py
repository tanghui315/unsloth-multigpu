"""
DDP Launcher - åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨å™¨
åŸºäºtorch.multiprocessingå®ç°å¤šè¿›ç¨‹DDPè®­ç»ƒ
"""

import logging
import os
import sys
from typing import Any, Callable, Dict, Optional

import torch
import torch.multiprocessing as mp
from torch.multiprocessing import Process

from .ddp_manager import setup_ddp_environment
from .ddp_trainer import DDPTrainer
from ..utils import MultiGPUConfig

logger = logging.getLogger(__name__)


class DDPLauncher:
    """
    DDP Launcher - åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨å™¨
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. ç®¡ç†å¤šè¿›ç¨‹è®­ç»ƒå¯åŠ¨
    2. é”™è¯¯å¤„ç†å’Œè¿›ç¨‹ç›‘æ§
    3. ç»“æœæ”¶é›†å’Œæ±‡æ€»
    4. èµ„æºæ¸…ç†
    """
    
    def __init__(self, config: MultiGPUConfig):
        """
        åˆå§‹åŒ–DDPå¯åŠ¨å™¨
        
        Args:
            config: å¤šGPUé…ç½®
        """
        self.config = config
        self.processes = []
        self.results = {}
        
        logger.info(f"ğŸ”§ åˆå§‹åŒ–DDPLauncher: {config.num_gpus} GPUs")
    
    def launch_training(self, 
                       trainer_fn: Callable,
                       trainer_args: tuple = (),
                       trainer_kwargs: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
        
        Args:
            trainer_fn: è®­ç»ƒå‡½æ•°
            trainer_args: è®­ç»ƒå‡½æ•°ä½ç½®å‚æ•°
            trainer_kwargs: è®­ç»ƒå‡½æ•°å…³é”®å­—å‚æ•°
            
        Returns:
            Dict: è®­ç»ƒç»“æœ
        """
        if trainer_kwargs is None:
            trainer_kwargs = {}
            
        logger.info(f"ğŸš€ å¯åŠ¨DDPè®­ç»ƒ: {self.config.num_gpus} è¿›ç¨‹")
        
        try:
            # è®¾ç½®DDPç¯å¢ƒ
            setup_ddp_environment(self.config.num_gpus)
            
            # ä½¿ç”¨spawnæ–¹æ³•å¯åŠ¨è¿›ç¨‹ï¼ˆæ¨èç”¨äºCUDAï¼‰
            mp.set_start_method('spawn', force=True)
            
            # åˆ›å»ºè¿›ç¨‹
            processes = []
            for rank in range(self.config.num_gpus):
                p = Process(
                    target=self._worker_process,
                    args=(rank, trainer_fn, trainer_args, trainer_kwargs)
                )
                p.start()
                processes.append(p)
            
            # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
            for p in processes:
                p.join()
            
            # æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
            success_count = 0
            for rank, p in enumerate(processes):
                if p.exitcode == 0:
                    success_count += 1
                else:
                    logger.error(f"âŒ è¿›ç¨‹ {rank} é€€å‡ºå¼‚å¸¸: exit_code={p.exitcode}")
            
            if success_count == self.config.num_gpus:
                logger.info("âœ… æ‰€æœ‰DDPè¿›ç¨‹æˆåŠŸå®Œæˆ")
                return {'status': 'success', 'num_processes': success_count}
            else:
                logger.error(f"âŒ DDPè®­ç»ƒå¤±è´¥: {success_count}/{self.config.num_gpus} è¿›ç¨‹æˆåŠŸ")
                return {'status': 'failed', 'success_count': success_count}
                
        except Exception as e:
            logger.error(f"âŒ DDPå¯åŠ¨å¤±è´¥: {e}")
            self._cleanup_processes()
            raise
    
    def _worker_process(self, 
                       rank: int, 
                       trainer_fn: Callable,
                       trainer_args: tuple,
                       trainer_kwargs: Dict[str, Any]):
        """
        å·¥ä½œè¿›ç¨‹å‡½æ•°
        
        Args:
            rank: è¿›ç¨‹rank
            trainer_fn: è®­ç»ƒå‡½æ•°
            trainer_args: è®­ç»ƒå‡½æ•°å‚æ•°
            trainer_kwargs: è®­ç»ƒå‡½æ•°å…³é”®å­—å‚æ•°
        """
        try:
            # è®¾ç½®è¿›ç¨‹æ—¥å¿—
            process_logger = logging.getLogger(f"ddp_worker_{rank}")
            process_logger.info(f"ğŸš€ å¯åŠ¨DDPå·¥ä½œè¿›ç¨‹ (rank={rank})")
            
            # è°ƒç”¨è®­ç»ƒå‡½æ•°
            result = trainer_fn(rank, *trainer_args, **trainer_kwargs)
            
            process_logger.info(f"âœ… DDPå·¥ä½œè¿›ç¨‹å®Œæˆ (rank={rank})")
            return result
            
        except Exception as e:
            logger.error(f"âŒ DDPå·¥ä½œè¿›ç¨‹å¤±è´¥ (rank={rank}): {e}")
            logger.error(f"å¼‚å¸¸è¯¦æƒ…:", exc_info=True)
            sys.exit(1)
    
    def _cleanup_processes(self):
        """æ¸…ç†è¿›ç¨‹èµ„æº"""
        for p in self.processes:
            if p.is_alive():
                p.terminate()
                p.join(timeout=5.0)
                if p.is_alive():
                    p.kill()
        
        self.processes.clear()
        logger.info("ğŸ§¹ è¿›ç¨‹èµ„æºæ¸…ç†å®Œæˆ")


def ddp_train_worker(rank: int, 
                    original_trainer,
                    config: MultiGPUConfig) -> Dict[str, Any]:
    """
    DDPè®­ç»ƒå·¥ä½œå‡½æ•°
    
    Args:
        rank: è¿›ç¨‹rank
        original_trainer: åŸå§‹trainer
        config: å¤šGPUé…ç½®
        
    Returns:
        Dict: è®­ç»ƒç»“æœ
    """
    from .ddp_manager import DDPManager
    
    try:
        # åˆ›å»ºDDPç®¡ç†å™¨å’Œè®­ç»ƒå™¨
        ddp_manager = DDPManager()
        ddp_trainer = DDPTrainer(original_trainer, config, ddp_manager)
        
        # è®¾ç½®DDPç¯å¢ƒ
        ddp_trainer.setup(rank)
        
        # æ‰§è¡Œè®­ç»ƒ
        result = ddp_trainer.train()
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ DDPè®­ç»ƒå·¥ä½œè¿›ç¨‹å¤±è´¥ (rank={rank}): {e}")
        raise


def launch_ddp_training(original_trainer, config: MultiGPUConfig) -> Dict[str, Any]:
    """
    ä¾¿æ·çš„DDPè®­ç»ƒå¯åŠ¨å‡½æ•°
    
    Args:
        original_trainer: åŸå§‹HuggingFace trainer
        config: å¤šGPUé…ç½®
        
    Returns:
        Dict: è®­ç»ƒç»“æœ
    """
    launcher = DDPLauncher(config)
    
    return launcher.launch_training(
        trainer_fn=ddp_train_worker,
        trainer_args=(original_trainer, config)
    )