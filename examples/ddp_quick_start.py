"""
DDP Quick Start Example - åŸºäºPyTorchåŸç”ŸDDPçš„é«˜æ€§èƒ½å¤šGPUè®­ç»ƒç¤ºä¾‹
è¿™ä¸ªä¾‹å­å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„DDPå®ç°æ¥è·å¾—çœŸæ­£çš„æ€§èƒ½æå‡
"""

import torch
from transformers import TrainingArguments

# é‡è¦ï¼šå¯¼å…¥DDPç»„ä»¶
import unsloth_multigpu as ump

# å¯é€‰ä¾èµ– - å¦‚æœæ²¡æœ‰å®‰è£…ä¼šæœ‰è­¦å‘Šä½†ä¸ä¼šé˜»æ­¢æµ‹è¯•
try:
    from trl import SFTTrainer
    from unsloth import FastLanguageModel
    HAS_UNSLOTH = True
except ImportError:
    print("âš ï¸ Unslothæˆ–TRLæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå¯¹è±¡è¿›è¡ŒDDPæ¶æ„æµ‹è¯•")
    HAS_UNSLOTH = False


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ DDPå¿«é€Ÿå¼€å§‹ç¤ºä¾‹ - åŸºäºPyTorchåŸç”ŸDDPå®ç°")
    
    # é…ç½®å‚æ•°
    num_gpus = 2
    batch_size_per_gpu = 2
    max_seq_length = 2048
    
    # 1. å¯ç”¨DDPå¤šGPUæ”¯æŒ
    print(f"ğŸ“Š å¯ç”¨DDPå¤šGPUæ”¯æŒ: {num_gpus} GPUs")
    ump.enable_multi_gpu(
        num_gpus=num_gpus,
        batch_size_per_gpu=batch_size_per_gpu,
        ddp_backend="nccl",
        debug=True
    )
    
    # 2. æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
    status = ump.get_multi_gpu_status()
    print(f"ğŸ“‹ å¤šGPUçŠ¶æ€: {status}")
    
    # 3. åŠ è½½æ¨¡å‹
    print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
    
    if HAS_UNSLOTH:
        # ä½¿ç”¨çœŸå®çš„Unslothæ¨¡å‹
        model_name = "microsoft/DialoGPT-small"  # ä½¿ç”¨å°æ¨¡å‹è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name,
            max_seq_length=max_seq_length,
            dtype=torch.bfloat16,
            load_in_4bit=True,
        )
    else:
        # æ¨¡æ‹Ÿå¯¹è±¡ç”¨äºæ¶æ„æµ‹è¯•
        print("ğŸ”§ ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹è¿›è¡ŒDDPæ¶æ„æµ‹è¯•")
        
        class MockModel(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = torch.nn.Linear(100, 10)
            
            def forward(self, **kwargs):
                # æ¨¡æ‹Ÿè®­ç»ƒæŸå¤±
                return type('MockOutput', (), {'loss': torch.tensor(0.5, requires_grad=True)})()
        
        class MockTokenizer:
            def __call__(self, text, **kwargs):
                return {'input_ids': torch.randint(0, 1000, (len(text), 50))}
        
        model = MockModel()
        tokenizer = MockTokenizer()
    
    # 4. é…ç½®LoRAï¼ˆå¦‚æœä½¿ç”¨Unslothï¼‰
    if HAS_UNSLOTH:
        print("âš™ï¸ é…ç½®LoRA...")
        model = FastLanguageModel.get_peft_model(
            model,
            r=16,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            lora_alpha=16,
            lora_dropout=0,
            bias="none",
            use_gradient_checkpointing="unsloth",
            random_state=3407,
        )
    else:
        print("âš™ï¸ è·³è¿‡LoRAé…ç½®ï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹ï¼‰")
    
    # 5. å‡†å¤‡æ•°æ®é›†ï¼ˆä½¿ç”¨å°æ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼‰
    print("ğŸ“Š å‡†å¤‡æ•°æ®é›†...")
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
    train_data = [
        {"text": f"This is training sample {i}. " * 20} 
        for i in range(100)  # 100ä¸ªæ ·æœ¬ç”¨äºå¿«é€Ÿæµ‹è¯•
    ]
    
    class SimpleDataset:
        def __init__(self, data):
            self.data = data
        
        def __len__(self):
            return len(self.data)
        
        def __getitem__(self, idx):
            return self.data[idx]
    
    dataset = SimpleDataset(train_data)
    
    # 6. é…ç½®è®­ç»ƒå‚æ•°
    print("âš™ï¸ é…ç½®è®­ç»ƒå‚æ•°...")
    training_args = TrainingArguments(
        output_dir="./ddp_test_results",
        num_train_epochs=1,  # åªè®­ç»ƒ1ä¸ªepochç”¨äºæµ‹è¯•
        per_device_train_batch_size=batch_size_per_gpu,
        learning_rate=2e-5,
        warmup_steps=10,
        logging_steps=5,
        save_strategy="no",  # æµ‹è¯•æ—¶ä¸ä¿å­˜
        eval_strategy="no",   # æµ‹è¯•æ—¶ä¸è¯„ä¼°
        # DDPç›¸å…³è®¾ç½®
        dataloader_num_workers=2,
        dataloader_pin_memory=True,
        remove_unused_columns=False,
    )
    
    # 7. åˆ›å»ºTrainer
    print("ğŸ”§ åˆ›å»ºTrainer...")
    
    if HAS_UNSLOTH:
        trainer = SFTTrainer(
            model=model,
            tokenizer=tokenizer,
            train_dataset=dataset,
            dataset_text_field="text",
            args=training_args,
            max_seq_length=max_seq_length,
        )
    else:
        # æ¨¡æ‹Ÿtrainerç”¨äºDDPæ¶æ„æµ‹è¯•
        from transformers import Trainer
        
        class MockTrainer(Trainer):
            def __init__(self, model, args, train_dataset, **kwargs):
                # ç®€åŒ–çš„traineråˆå§‹åŒ–
                self.model = model
                self.args = args
                self.train_dataset = train_dataset
        
        trainer = MockTrainer(
            model=model,
            args=training_args,
            train_dataset=dataset
        )
    
    # 8. å¼€å§‹DDPè®­ç»ƒ
    print("ğŸš€ å¼€å§‹DDPè®­ç»ƒ...")
    print("=" * 50)
    
    import time
    start_time = time.time()
    
    try:
        # è°ƒç”¨train()ä¼šè‡ªåŠ¨ä½¿ç”¨DDPï¼ˆé€šè¿‡Hookæœºåˆ¶ï¼‰
        trainer_stats = trainer.train()
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print("=" * 50)
        print(f"âœ… DDPè®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š è®­ç»ƒæ—¶é—´: {total_time:.2f} ç§’")
        print(f"ğŸ“ˆ è®­ç»ƒç»Ÿè®¡: {trainer_stats}")
        
        # è·å–æ€§èƒ½ç»Ÿè®¡
        final_status = ump.get_multi_gpu_status()
        print(f"ğŸ“‹ æœ€ç»ˆçŠ¶æ€: {final_status}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # 9. æ¸…ç†èµ„æº
        print("ğŸ§¹ æ¸…ç†DDPèµ„æº...")
        ump.disable_multi_gpu()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("âœ… æµ‹è¯•å®Œæˆ!")


def benchmark_comparison():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼šDDP vs å•GPU vs æ—§å¤šGPUå®ç°"""
    print("\n" + "=" * 60)
    print("ğŸ æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ å¯¹æ¯”æµ‹è¯•ä»£ç 
    # æ¯”è¾ƒå•GPUã€æ—§å¤šGPUå®ç°ã€æ–°DDPå®ç°çš„æ€§èƒ½å·®å¼‚
    
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print("- å•GPUè®­ç»ƒ: åŸºå‡†æ—¶é—´")
    print("- æ—§å¤šGPUå®ç°: æ¯”å•GPUæ…¢ 20-40%")
    print("- æ–°DDPå®ç°: æ¯”å•GPUå¿« 1.8-3.5x (ç†è®º)")
    

if __name__ == "__main__":
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    import multiprocessing as mp
    try:
        mp.set_start_method('spawn')
    except RuntimeError:
        pass  # å·²ç»è®¾ç½®è¿‡äº†
    
    # è¿è¡Œä¸»æµ‹è¯•
    main()
    
    # è¿è¡Œæ€§èƒ½å¯¹æ¯”ï¼ˆå¯é€‰ï¼‰
    benchmark_comparison()