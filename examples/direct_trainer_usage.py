"""
Direct usage example of MultiGPUTrainer
Demonstrates how to bypass the Hook mechanism and use our Multi-GPU trainer directly
"""

import os

import torch
from datasets import load_dataset
from torch.utils.data import DataLoader
from transformers import TrainingArguments

from unsloth import FastLanguageModel
# ç›´æ¥å¯¼å…¥æˆ‘ä»¬çš„æ ¸å¿ƒç»„ä»¶
from unsloth_multigpu_prototype.core import AggregationMethod, MultiGPUTrainer
from unsloth_multigpu_prototype.utils import (ConfigManager, DeviceManager,
                                              MultiGPULogger)


def create_dataloader(dataset, tokenizer, batch_size=8):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    def tokenize_function(examples):
        # ç®€åŒ–çš„åˆ†è¯å‡½æ•°
        inputs = tokenizer(
            examples['instruction'], 
            truncation=True, 
            padding=True, 
            max_length=512,
            return_tensors="pt"
        )
        # æ·»åŠ æ ‡ç­¾ï¼ˆåœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œæ‚¨å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å¤„ç†ï¼‰
        inputs['labels'] = inputs['input_ids'].clone()
        return inputs
    
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    
    return DataLoader(
        tokenized_dataset, 
        batch_size=batch_size, 
        shuffle=True,
        collate_fn=lambda x: {k: torch.stack([item[k] for item in x]) for k in x[0]}
    )

def main():
    """ç›´æ¥ä½¿ç”¨MultiGPUTrainerçš„ä¸»å‡½æ•°"""
    
    # 1. åˆå§‹åŒ–è®¾å¤‡å’Œé…ç½®ç®¡ç†
    print("ğŸ”§ åˆå§‹åŒ–é…ç½®...")
    device_manager = DeviceManager()
    config_manager = ConfigManager()
    logger = MultiGPULogger(log_dir="./logs/direct_trainer")
    
    # æ£€æŸ¥å¯ç”¨è®¾å¤‡
    available_devices = device_manager.get_available_devices()
    num_gpus = min(len(available_devices), 4)  # æœ€å¤šä½¿ç”¨4ä¸ªGPU
    
    if num_gpus < 2:
        print("âš ï¸ å¤šGPUè®­ç»ƒéœ€è¦è‡³å°‘2ä¸ªGPUï¼Œå½“å‰ç¯å¢ƒåªèƒ½è¿›è¡Œæ¼”ç¤º")
        num_gpus = 1  # é™çº§åˆ°å•GPUæ¨¡å¼
    
    print(f"ğŸ“Š ä½¿ç”¨ {num_gpus} ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
    
    # 2. åŠ è½½æ¨¡å‹
    print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        "unsloth/llama-2-7b-bnb-4bit",
        max_seq_length=2048,
        dtype="bfloat16",
        load_in_4bit=True
    )
    
    # 3. å‡†å¤‡æ•°æ®
    print("ğŸ“š å‡†å¤‡æ•°æ®é›†...")
    dataset = load_dataset("tatsu-lab/alpaca", split="train[:100]")  # ä½¿ç”¨å°æ•°æ®é›†æ¼”ç¤º
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = create_dataloader(dataset, tokenizer, batch_size=4)
    
    # 4. é…ç½®ä¼˜åŒ–å™¨
    optimizer_config = {
        'class': torch.optim.AdamW,
        'kwargs': {
            'lr': 2e-5,
            'weight_decay': 0.01,
            'betas': (0.9, 0.999)
        }
    }
    
    # 5. åˆ›å»ºMultiGPUTrainer
    print("ğŸš€ åˆå§‹åŒ–MultiGPUTrainer...")
    trainer = MultiGPUTrainer(
        model=model,
        num_gpus=num_gpus,
        optimizer_config=optimizer_config,
        aggregation_method=AggregationMethod.MEAN,
        enable_gradient_checkpointing=True,
        enable_mixed_precision=True
    )
    
    # 6. è®¾ç½®è®­ç»ƒå™¨
    trainer.setup()
    
    # 7. å¼€å§‹è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    
    try:
        # è®­ç»ƒå¤šä¸ªepoch
        for epoch in range(2):  # è®­ç»ƒ2ä¸ªepochæ¼”ç¤º
            print(f"\nğŸ“– Epoch {epoch + 1}/2")
            
            epoch_stats = trainer.train_epoch(
                dataloader=dataloader,
                max_steps=10  # é™åˆ¶æ­¥æ•°ç”¨äºæ¼”ç¤º
            )
            
            print(f"âœ… Epoch {epoch + 1} å®Œæˆ:")
            print(f"  - å¹³å‡æŸå¤±: {epoch_stats['avg_loss']:.4f}")
            print(f"  - è®­ç»ƒæ­¥æ•°: {epoch_stats['steps']}")
            print(f"  - å¤„ç†æ ·æœ¬: {epoch_stats['samples']}")
            print(f"  - è€—æ—¶: {epoch_stats['epoch_time']:.2f}s")
        
        # 8. è·å–è®­ç»ƒç»Ÿè®¡
        print("\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        training_stats = trainer.get_training_stats()
        
        print(f"  - æ€»æ­¥æ•°: {training_stats['global_step']}")
        print(f"  - å¤„ç†æ‰¹æ¬¡: {training_stats['num_batches_processed']}")
        print(f"  - å¤„ç†æ ·æœ¬: {training_stats['num_samples_processed']}")
        print(f"  - å¹³å‡å‰å‘æ—¶é—´: {training_stats.get('avg_forward_time_ms', 0):.2f}ms")
        print(f"  - å¹³å‡åå‘æ—¶é—´: {training_stats.get('avg_backward_time_ms', 0):.2f}ms")
        print(f"  - å¹³å‡èšåˆæ—¶é—´: {training_stats.get('avg_aggregation_time_ms', 0):.2f}ms")
        
        # 9. ä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint_path = "./checkpoints/direct_trainer_checkpoint.pt"
        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
        trainer.save_checkpoint(checkpoint_path)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # 10. æ¸…ç†èµ„æº
        print("ğŸ§¹ æ¸…ç†èµ„æº...")
        trainer.cleanup()
        print("âœ… ç›´æ¥ä½¿ç”¨MultiGPUTrainerç¤ºä¾‹å®Œæˆ!")

def compare_approaches():
    """æ¯”è¾ƒä¸¤ç§ä½¿ç”¨æ–¹å¼"""
    print("\n" + "="*60)
    print("ğŸ“‹ ä¸¤ç§ä½¿ç”¨æ–¹å¼å¯¹æ¯”")
    print("="*60)
    
    print("\nğŸ”§ æ–¹å¼1: Hookæœºåˆ¶ (quick_start.py)")
    print("  ä¼˜ç‚¹:")
    print("    âœ… é›¶ä¾µå…¥æ€§ - ç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹")
    print("    âœ… å‘åå…¼å®¹ - å®Œå…¨å…¼å®¹Unsloth API")
    print("    âœ… ç®€å•æ˜“ç”¨ - åªéœ€enable_multi_gpu()")
    print("  ç¼ºç‚¹:")
    print("    âš ï¸ ä¸å¤Ÿç›´è§‚ - ç”¨æˆ·å¯èƒ½ä¸çŸ¥é“å†…éƒ¨æœºåˆ¶")
    print("    âš ï¸ è°ƒè¯•å¤æ‚ - Hookæ›¿æ¢å¯èƒ½éš¾ä»¥è°ƒè¯•")
    
    print("\nğŸ¯ æ–¹å¼2: ç›´æ¥ä½¿ç”¨ (æœ¬ç¤ºä¾‹)")
    print("  ä¼˜ç‚¹:")
    print("    âœ… å®Œå…¨æ§åˆ¶ - ç”¨æˆ·æ˜ç¡®çŸ¥é“ä½¿ç”¨çš„æ˜¯ä»€ä¹ˆ")
    print("    âœ… æ˜“äºè°ƒè¯• - ç›´æ¥è®¿é—®æ‰€æœ‰ç»„ä»¶")
    print("    âœ… çµæ´»é…ç½® - å¯ä»¥ç²¾ç»†è°ƒæ•´æ‰€æœ‰å‚æ•°")
    print("  ç¼ºç‚¹:")
    print("    âš ï¸ éœ€è¦å­¦ä¹  - ç”¨æˆ·éœ€è¦äº†è§£æ–°API")
    print("    âš ï¸ ä»£ç ä¿®æ”¹ - éœ€è¦ä¿®æ”¹ç°æœ‰è®­ç»ƒä»£ç ")
    
    print("\nğŸ’¡ å»ºè®®:")
    print("  - å¿«é€Ÿè¿ç§»: ä½¿ç”¨Hookæœºåˆ¶")
    print("  - æ–°é¡¹ç›®/é«˜çº§ç”¨æˆ·: ç›´æ¥ä½¿ç”¨MultiGPUTrainer")
    print("  - è°ƒè¯•/å®šåˆ¶: ç›´æ¥ä½¿ç”¨MultiGPUTrainer")

if __name__ == "__main__":
    main()
    compare_approaches() 